{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORPUS AUGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides the code used for generating an augmented corpus with more German words. Google Translate API was used to translate English->German."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global modules import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loading import create_word_lists, tidy_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/corpus_data.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "data = data[\"records\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_transcripts = [entry[\"human_transcript\"] for entry in data]\n",
    "stt_transcripts = [entry[\"stt_transcript\"] for entry in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_words, stt_words, word_labels, word_grams, word_sems = create_word_lists(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the sentences are too long, so we need to shorten them. The sentences are basically concatenations of individual words with spaces in between, without any interpuction, so they are reconstructed from word lists when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stt_transcripts, stt_words, word_labels, word_grams, word_sems = tidy_sentence_length(\n",
    "    stt_transcripts, stt_words, word_labels, word_grams, word_sems\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(map(len, word_labels))\n",
    "padded_labels = [row + [False] * (max_length - len(row)) for row in word_labels]\n",
    "padded_labels = np.array(padded_labels)\n",
    "stat_labels = np.any(padded_labels, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we split only indices and not data itself, because the data contains arrays of variable length, which does not work with `train_test_split`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(stt_transcripts)))\n",
    "tr_indices, te_indices = train_test_split(\n",
    "    indices, test_size=0.2, random_state=0, shuffle=True, stratify=stat_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are hepler functions that will extract data selected by indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_train = itemgetter(*tr_indices)\n",
    "extract_test = itemgetter(*te_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, do data splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_stt_transcripts = extract_train(stt_transcripts)\n",
    "tr_stt_words = extract_train(stt_words)\n",
    "\n",
    "tr_word_labels = extract_train(word_labels)\n",
    "tr_word_grams = extract_train(word_grams)\n",
    "tr_word_sems = extract_train(word_sems)\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "te_stt_transcripts = extract_test(stt_transcripts)\n",
    "te_stt_words = extract_test(stt_words)\n",
    "\n",
    "te_word_labels = extract_test(word_labels)\n",
    "te_word_grams = extract_test(word_grams)\n",
    "te_word_sems = extract_test(word_sems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate English Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we choose which part of the corpus to translate. Instead of using `tr_` variables, `te_` variables can be chosen to translate test words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tr_words, new_tr_word_labels = ([], [])\n",
    "to_translate_words = []\n",
    "\n",
    "for sentence, labels in zip(tr_stt_words, tr_word_labels):\n",
    "    if any(labels):\n",
    "        new_tr_words.append(sentence)\n",
    "        new_tr_word_labels.append(labels)\n",
    "    else:\n",
    "        to_translate_words.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translation import translate_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the translation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4676/4676 [29:44<00:00,  2.62it/s]  \n"
     ]
    }
   ],
   "source": [
    "translations, tr_labels, n = translate_sentences(to_translate_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the proportion of failed translations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5241659538066724\n"
     ]
    }
   ],
   "source": [
    "print(n / len(to_translate_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary, translate can be used again on a newly created dataset to further increase the proportion of German words. API call fails seem unavoidable, and by repeatedly translating the same dataset results can be made better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the original German sentences and the augmented part of dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tr_words = new_tr_words + translations\n",
    "new_tr_word_labels = new_tr_word_labels + tr_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"intermediate_data/translator_basic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(out_path, \"words_higher_perc.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(new_tr_words, file)\n",
    "with open(os.path.join(out_path, \"labels_higher_perc.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(new_tr_word_labels, file)\n",
    "with open(os.path.join(out_path, \"test_words_higher_perc.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(te_stt_words, file)\n",
    "with open(os.path.join(out_path, \"test_labels_higher_perc.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(te_word_labels, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can aslo check the proportion of German words in the augmented set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11717936393495153\n"
     ]
    }
   ],
   "source": [
    "num_words = 0\n",
    "num_germans = 0\n",
    "for l in new_tr_word_labels:\n",
    "    num_words += len(l)\n",
    "    num_germans += sum(l)\n",
    "print(num_germans / num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the original one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.029981542412326458\n"
     ]
    }
   ],
   "source": [
    "num_words = 0\n",
    "num_germans = 0\n",
    "for l in word_labels:\n",
    "    num_words += len(l)\n",
    "    num_germans += sum(l)\n",
    "print(num_germans / num_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
