{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global modules import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local modules import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loading import create_word_lists, tidy_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/corpus_data.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "data = data['records']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_transcripts = [entry['human_transcript'] for entry in data]\n",
    "stt_transcripts   = [entry['stt_transcript'] for entry in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_words, stt_words, word_labels, word_grams, word_sems = \\\n",
    "    create_word_lists(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the sentences are too long, so we need to shorten them. The sentences are basically concatenations of individual words with spaces in between, without any interpuction, so they are reconstructed from word lists when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stt_transcripts, stt_words, word_labels, word_grams, word_sems = \\\n",
    "    tidy_sentence_length(stt_transcripts, stt_words, word_labels, word_grams, word_sems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(map(len, word_labels))\n",
    "padded_labels = [row + [False] * (max_length - len(row)) for row in word_labels]\n",
    "padded_labels = np.array(padded_labels)\n",
    "stat_labels = np.any(padded_labels, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we split only indices and not data itself, because the data contains arrays of variable length, which does not work with `train_test_split`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(stt_transcripts)))\n",
    "tr_indices, te_indices = train_test_split(indices, test_size=0.2, random_state=0, shuffle=True, stratify=stat_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are hepler functions that will extract data selected by indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_train = itemgetter(*tr_indices)\n",
    "extract_test  = itemgetter(*te_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, do data splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_stt_transcripts   = extract_train(stt_transcripts)\n",
    "tr_stt_words         = extract_train(stt_words)\n",
    "\n",
    "tr_word_labels       = extract_train(word_labels)\n",
    "tr_word_grams        = extract_train(word_grams)\n",
    "tr_word_sems         = extract_train(word_sems)\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "te_stt_transcripts   = extract_test(stt_transcripts)\n",
    "te_stt_words         = extract_test(stt_words)\n",
    "\n",
    "te_word_labels       = extract_test(word_labels)\n",
    "te_word_grams        = extract_test(word_grams)\n",
    "te_word_sems         = extract_test(word_sems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tr_words, new_tr_word_labels = ([], [])\n",
    "to_translate_words = []\n",
    "\n",
    "for sentence, labels in zip(tr_stt_words, tr_word_labels):\n",
    "    if any(labels):\n",
    "        new_tr_words.append(sentence)\n",
    "        new_tr_word_labels.append(labels)\n",
    "    else:\n",
    "        to_translate_words.append(sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rnd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentences(sentences, n_untranslated=0):\n",
    "    translator = Translator()\n",
    "    translations, tr_labels = ([], [])\n",
    "\n",
    "    for sentence in tqdm(sentences):\n",
    "\n",
    "        if rnd.random() < 0.1:\n",
    "            translations.append(sentence)\n",
    "            tr_labels.append([0]*len(sentence))\n",
    "            continue\n",
    "        \n",
    "        new_sentence = []\n",
    "        new_labels = []\n",
    "\n",
    "        for word in sentence:\n",
    "            if rnd.random() < 0.2:\n",
    "                try:\n",
    "                    new_sentence.append(translator.translate(word, src='en', dest='de').text)\n",
    "                    new_labels.append(1)\n",
    "                except:\n",
    "                    n_untranslated+=1\n",
    "                    new_sentence.append(word)\n",
    "                    new_labels.append(0)\n",
    "            else:\n",
    "                new_sentence.append(word)\n",
    "                new_labels.append(0)\n",
    "        \n",
    "        translations.append(new_sentence)\n",
    "        tr_labels.append(new_labels)\n",
    "        \n",
    "\n",
    "    return translations, tr_labels, n_untranslated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4676/4676 [29:44<00:00,  2.62it/s]  \n"
     ]
    }
   ],
   "source": [
    "translations, tr_labels, n = translate_sentences(to_translate_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5241659538066724\n"
     ]
    }
   ],
   "source": [
    "print(n/len(to_translate_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tr_words = new_tr_words + translations\n",
    "new_tr_word_labels = new_tr_word_labels + tr_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(new_tr_words) == len(new_tr_word_labels))\n",
    "for w, l in zip(new_tr_words, new_word_labels):\n",
    "    if len(w)!=len(l):\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = '../intermediate_data/translator_basic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(out_path, 'words_higher_perc.pkl'), 'wb') as file:\n",
    "    pickle.dump(new_tr_words, file)\n",
    "with open(os.path.join(out_path, 'labels_higher_perc.pkl'), 'wb') as file:\n",
    "    pickle.dump(new_tr_word_labels, file)\n",
    "with open(os.path.join(out_path, 'test_words_higher_perc.pkl'), 'wb') as file:\n",
    "    pickle.dump(te_stt_words, file)\n",
    "with open(os.path.join(out_path, 'test_labels_higher_perc.pkl'), 'wb') as file:\n",
    "    pickle.dump(te_word_labels, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11717936393495153\n"
     ]
    }
   ],
   "source": [
    "num_words = 0\n",
    "num_germans = 0\n",
    "for l in new_tr_word_labels:\n",
    "    num_words += len(l)\n",
    "    num_germans += sum(l)\n",
    "print(num_germans/num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.029981542412326458\n"
     ]
    }
   ],
   "source": [
    "num_words = 0\n",
    "num_germans = 0\n",
    "for l in word_labels:\n",
    "    num_words += len(l)\n",
    "    num_germans += sum(l)\n",
    "print(num_germans/num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0'], ['when', 'i', 'hang', 'on', 'my', 'friends', 'ich', 'go', 'to', 'drink', 'a', 'beer', 'Und', 'then', 'we', 'go', 'to', 'the', 'beach', 'we', 'nehmen', 'a', 'swim', 'we', 'serf', 'and', 'Wir', 'gehen', 'eat', 'let', 'the', 'day', 'and', 'what', 'do', 'you', 'actually', 'do', 'in', 'you', 'of', 'what', 'do', 'you', 'do', 'actually', 'in', 'school', 'in', 'your', 'school'], ['i', 'can', 'see', '7', 'balls', 'of', 'the', 'picture'], ['and', 'what', 'is', 'your', 'favorite', 'food'], ['or', 'rink'], ['it', 'is', 'something', 'where', 'you', 'can', 'drive', 'or', 'you', 'can', 'ich'], ['my', 'name', 'is', 'Pluto', 'and', 'what', 'is', 'your', 'name'], ['the', 'people', 'has', 'blue', 'jeans', 'and', 'a', 'purple', 't', 'shirt'], ['is', 'it', 'a', 'reporter'], ['i', 'like', 'both', 'because', 'of', 'In', 'on', 'the', 'beach', 'you', 'can', 'you', 'can', 'swim', 'and', 'you', 'can', 'you', 'can', 'lie', 'in', 'the', 'sun', 'and', 'it', 'is', 'hot', 'there', 'but', 'ich', 'also', 'like', 'mountains', 'because', 'of', 'you', 'can', 'hide', 'there', 'and', 'it', 'is', 'cold', 'es'], ['we', 'have', 'lully', 'pup'], ['no'], ['200', 'that', 'my', 'motor', 'do', 'not', 'fell', 'out', 'the', 'game', 'so', 'yeah', 'my', 'sister', 'is', 'like', 'strong', 'so', 'it', 'is', 'wie', 'when', 'i', 'give', 'her', 'back', 'air', 'like', 'it', 'is', 'okay', 'for', '200', 'not', '300', 'she', 'say', 'like', 'okay', 'and', 'then', 'she', 'gives', 'me', '200', 'other', 'but', 'like', 'when', 'i', 'say', 'can', 'you', 'give', 'you', '150', 'and', 'not', '200', 'she', 'said', 'no', 'and', 'i', 'am', 'like', 'okay', 'then', 'okay', 'so', 'yeah'], ['where', 'where', 'are', 'you', 'from'], ['sofa', 'is', 'denser', 'sofa'], ['a', 'dog', 'tell', 'me', 'about', 'something', 'that', 'you', 'love'], ['i', 'am', 'from', 'happy', 'land', 'watch', 'about', 'you'], ['when', 'it', 'is', 'rain', 'we', 'are', 'half', 'at', 'the', 'top', 'on', 'the', 'head'], ['no'], ['you', 'know'], ['dancer', 'a', 'moon'], ['yes', 'so', 'i', 'want', '3', 'of', 'this'], ['yes'], ['by', 'and', 'anything', 'else', 'we', 'have', 'very', 'very', 'nice', 'lowing', 'books', 'we', 'have', 'chocolate', 'ones', 'which', 'the', 'which', 'there', 'is', 'only', 'here', 'we', 'have', 'strawberry', 'we', 'have', 'very', 'exclusive', 'cocacola', 'once', 'so', 'which', 'one', 'do', 'you', 'want'], ['clock'], ['fruit', 'because', 'it', 'is', 'an', 'human', 'well', 'you', 'would', 'rather', 'be', 'able', 'to', 'fly', 'or', 'be', 'invisible'], ['because', 'there', 'no', 'stuff', 'which', 'i', 'should', 'could', 'wish', 'but', 'i', '1st', 'had', 'to', 'think', 'about', 'what', 'it', 'really', 'would', 'be'], ['ship', 'with', 'the', 'huckins'], ['it', 'is', 'an', 'instrument', 'for', 'like', 'streams', 'and', 'you', 'can', 'play', 'on', 'it', 'on', 'different', 'ways', 'it', 'looks', 'a', 'little', 'like', 'a', 'shell', 'but', 'it', 'is', 'smaller', 'but', 'it', 'is', 'not', 'a', 'violent'], ['i', 'like', 'it', 'more', 'pirates', 'an', 'you'], ['the', 'sheik', 'its', 'yellow'], ['yes'], ['it', 'is', 'the', 'animal'], ['okay'], ['at', 'the', 'top', 'are', 'you', 'at', 'the', 'right', 'side', 'of', 'the', 'picture'], ['my', 'picture', 'have', 'brown', 'hair'], ['how', 'expensive', 'is', 'the', 'cake'], ['yes'], ['look', 'over', 'the', 'rats', 'that', 'had', 'a', 'vanilla', 'cake', 'by', 'by', 'im', 'going', 'to', 'eat'], ['i', 'can', 'see', 'a', 'orange', 'cat'], ['nice', 'to', 'meet', 'you'], ['now', 'talk', 'may', 'you', 'talks', 'the', 'rites', 'edits', 'than', 'he', 'had', 'in', 'man'], ['my', 'favorite', 'food', 'is', 'pizza', 'hawaii'], ['we', 'had', 'delicious'], ['okay', 'thank', 'you'], ['would', 'you', 'rather', 'be', 'able', 'to', 'fly', 'or', 'be', 'invisible'], ['yes'], ['and', 'it', 'is'], ['yes'], ['okay', 'i', 'am', 'live', 'in', 'her', 'way', 'what', 'do', 'you', 'love', 'to', 'do']]\n"
     ]
    }
   ],
   "source": [
    "print(new_tr_words[-50:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
