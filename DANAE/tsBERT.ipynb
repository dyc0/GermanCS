{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5acfafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameter grid\n",
    "learning_rates = [1e-4, 5e-5, 3e-5]\n",
    "batch_sizes = [16] #[16, 32]\n",
    "num_epochs = 4\n",
    "\n",
    "# select the window size\n",
    "window_size = 0\n",
    "\n",
    "# Danae is doing:\n",
    "#lr = [1e-4, 5e-5, 3e-5] + batch_size = [16] + num_epochs = 4 + window_size = 1\n",
    "\n",
    "# define name of the file containing the fine-tuning results\n",
    "output_filename = \"tSBERT_finetuning_results_danae_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0077f87f",
   "metadata": {},
   "source": [
    "# Predicting german words using tsBERT (TongueSwitcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "375b4f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bed1ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import random as rnd\n",
    "from tqdm import tqdm\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import softmax\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3077af35",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67df4199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local modules import\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa3607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loading import create_word_lists, tidy_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44499252",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/corpus_data.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "records = data['records']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe5332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_transcripts = [entry['human_transcript'] for entry in records]\n",
    "stt_transcripts   = [entry['stt_transcript'] for entry in records]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6104a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# human-transcribed words, STT transcribed words, language labels, semantical errors, grammatical errors\n",
    "human_words, stt_words, word_labels, word_grams, word_sems = \\\n",
    "    create_word_lists(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fbc9501",
   "metadata": {},
   "outputs": [],
   "source": [
    "stt_transcripts, stt_words, labels, word_grams, word_sems = \\\n",
    "    tidy_sentence_length(stt_transcripts, stt_words, word_labels, word_grams, word_sems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd8cf601",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [[1 if label else 0 for label in record_labels] for record_labels in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea0698",
   "metadata": {},
   "source": [
    "## Quick Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9af919bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6723 records in total.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(records)} records in total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c01a46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words_per_record = [len(record['words']) for record in records]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17d432f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum number of words in any transcript is 1.\n",
      "The maximum number of words in any transcript is 517.\n",
      "The median number of words across all transcripts is 6.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The minimum number of words in any transcript is {min(nb_words_per_record)}.\")\n",
    "print(f\"The maximum number of words in any transcript is {max(nb_words_per_record)}.\")\n",
    "print(f\"The median number of words across all transcripts is {int(np.median(nb_words_per_record))}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c75c4c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distribution of word counts\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.displot(nb_words_per_record, orientation='horizontal', height=3, aspect=3)\n",
    "plt.xlim(1, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeb5a483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_label_proportions(labels):\n",
    "    total_tokens = sum(len(label_list) for label_list in labels)\n",
    "    german_tokens = sum(label for label_list in labels for label in label_list)\n",
    "    german_proportion = german_tokens / total_tokens\n",
    "    return german_proportion, total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9249423c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 62305 words in total out of which only 3.00% are german.\n"
     ]
    }
   ],
   "source": [
    "german_proportion, total_tokens = calculate_label_proportions(word_labels)\n",
    "print(f\"There are {total_tokens} words in total out of which only {german_proportion*100:.2f}% are german.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829d467c",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Data Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3179f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tsBERT model and the tokenizer\n",
    "model_name = \"igorsterner/german-english-code-switching-identification\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebf2488",
   "metadata": {},
   "source": [
    "### Incorporating Artificial Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309e2f75",
   "metadata": {},
   "source": [
    "=> TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a84340e",
   "metadata": {},
   "source": [
    "### Transcripts Processing\n",
    "--> how individual records are prepared for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8169c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contextual_input_with_labels(transcripts, labels, window_size=1, sep_token='[SEP]'):\n",
    "    \"\"\"\n",
    "    Create contextual input with labels, setting labels of context to -100.\n",
    "\n",
    "    Args:\n",
    "        transcripts (list of list of str): List of tokenized transcripts.\n",
    "        labels (list of list of int): List of labels for each transcript.\n",
    "        window_size (int): Size of the context window.\n",
    "        sep_token (str): Token used for separating sentences.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing modified transcripts and their corresponding labels.\n",
    "    \"\"\"\n",
    "    if window_size == 0:\n",
    "        return transcripts, labels\n",
    "\n",
    "    contextual_transcripts = []\n",
    "    contextual_labels = []\n",
    "\n",
    "    for i, (transcript, label) in enumerate(zip(transcripts, labels)):\n",
    "        # concatenate previous, current, and next transcripts within the window size\n",
    "        start_idx = max(i - window_size, 0)\n",
    "        end_idx = min(i + window_size + 1, len(transcripts))\n",
    "        \n",
    "        # prepare new transcript and labels\n",
    "        new_transcript = []\n",
    "        new_label = []\n",
    "\n",
    "        for j in range(start_idx, end_idx):\n",
    "            # intersperse the transcript with defined separator tokens ('.', [SEP], ...)\n",
    "            new_transcript.extend(transcripts[j] + [sep_token])\n",
    "            # extend labels with -100 for context transcripts, retain original for current\n",
    "            new_label.extend([-100] * len(transcripts[j]) if j != i else labels[j])\n",
    "            new_label.append(-100)  # for the SEP token\n",
    "\n",
    "        new_transcript.pop()  # remove the last SEP token\n",
    "        new_label.pop()       # remove the last label\n",
    "\n",
    "        contextual_transcripts.append(new_transcript)\n",
    "        contextual_labels.append(new_label)\n",
    "\n",
    "    return contextual_transcripts, contextual_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e9f5650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels(tokenized_inputs, input_labels):\n",
    "    \"\"\"\n",
    "    Preprocess labels based on tsBERT tokenization.\n",
    "\n",
    "    Args:\n",
    "        tokenized_inputs (transformers.TokenizerOutput): Tokenized inputs.\n",
    "        input_labels (list): Original labels for each input.\n",
    "\n",
    "    Returns:\n",
    "        list: Preprocessed labels for each tokenized input.\n",
    "    \"\"\"\n",
    "    tsBERT_labels = []\n",
    "\n",
    "    for i, _ in enumerate(tokenized_inputs.input_ids):\n",
    "        # get the word indices from original inputs\n",
    "        word_indices = tokenized_inputs.word_ids(batch_index=i)\n",
    "        # get the labels of the current record\n",
    "        record_labels = input_labels[i]\n",
    "        # initialize a list to store the new labels for this record\n",
    "        new_record_labels = []\n",
    "\n",
    "        for word_idx in word_indices:\n",
    "            # check if the word index is None => indicates a special token (e.g., [CLS], [SEP], [PAD])\n",
    "            if word_idx is None:\n",
    "                new_record_labels.append(-100)\n",
    "            # otherwise, retrieve the label corresponding to this word index from the record labels\n",
    "            else:\n",
    "                new_record_labels.append(record_labels[word_idx])\n",
    "\n",
    "        tsBERT_labels.append(new_record_labels)\n",
    "\n",
    "    return tsBERT_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cbe3b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the window size\n",
    "#window_size = 1\n",
    "\n",
    "# compute the contextual input with labels\n",
    "contextual_stt_input, contextual_labels = create_contextual_input_with_labels(stt_words, labels, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e13acaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the desired total length for BERT embeddings (to make sure they are the same between train-validation-test sets)\n",
    "max_length = 268  # set the desired total length\n",
    "\n",
    "# tokenize the STT-transcribed transcripts using the BERT tokenizer\n",
    "tokenized_inputs = tokenizer(contextual_stt_input, \n",
    "                             truncation=True, \n",
    "                             #padding=True,\n",
    "                             padding=\"max_length\", \n",
    "                             is_split_into_words=True,\n",
    "                             max_length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b5361",
   "metadata": {},
   "source": [
    "### Label Alignement\n",
    "- we adjust the labels: since we add context (neighbour transcripts) and [SEP] tokens as input the the model, we also also need to adjust out labels to align with the new token structure.\n",
    "- we assign an ignore index (-100) to these separator tokens since they don't carry meaningful language labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5eeeab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# align the labels so that they are suitable for the tsBERT model\n",
    "tsBERT_labels = preprocess_labels(tokenized_inputs, contextual_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6e7407",
   "metadata": {},
   "source": [
    "### Homophone-Enriched Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552a3a59",
   "metadata": {},
   "source": [
    "=> TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2510a2b2",
   "metadata": {},
   "source": [
    "## Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa5d59",
   "metadata": {},
   "source": [
    "We need to extract which sentences contain German words in order to stratify the data split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac99f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!\n",
    "#stt_transcripts = stt_transcripts[:500]\n",
    "#contextual_stt_input = contextual_stt_input[:500]\n",
    "#tsBERT_labels = tsBERT_labels[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5edb649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_labels = [[True if (label == 1) else False for label in record_labels] for record_labels in tsBERT_labels]\n",
    "stat_labels = np.any(stat_labels, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d13618a",
   "metadata": {},
   "source": [
    "Here, we split only indices and not data itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "244d8d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(stt_transcripts)))\n",
    "tr_indices, te_indices = train_test_split(indices, test_size=0.2, random_state=0, shuffle=True, stratify=stat_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f336d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_train = itemgetter(*tr_indices)\n",
    "extract_test  = itemgetter(*te_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bb4447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_contextual_stt_input = list(extract_train(contextual_stt_input))\n",
    "tr_tsBERT_labels        = list(extract_train(tsBERT_labels))\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "te_contextual_stt_input = list(extract_test(contextual_stt_input))\n",
    "te_tsBERT_labels        = list(extract_test(tsBERT_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2787f13",
   "metadata": {},
   "source": [
    "## tsBERT Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85e98584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STTDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf388a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the desired total length for BERT embeddings (to make sure they are the same between train-validation-test sets)\n",
    "max_length = 268  # set the desired total length\n",
    "\n",
    "train_inputs = tokenizer(tr_contextual_stt_input, \n",
    "                         truncation=True, \n",
    "                         #padding=True,\n",
    "                         padding=\"max_length\", \n",
    "                         is_split_into_words=True,\n",
    "                         max_length=max_length)\n",
    "#validation_inputs = tokenizer(val_contextual_stt_input, truncation=True, padding=True, is_split_into_words=True)\n",
    "validation_inputs = tokenizer(te_contextual_stt_input, \n",
    "                              truncation=True, \n",
    "                              padding=\"max_length\", \n",
    "                              is_split_into_words=True,\n",
    "                              max_length=max_length)\n",
    "test_inputs = tokenizer(te_contextual_stt_input, \n",
    "                        truncation=True, \n",
    "                        padding=\"max_length\", \n",
    "                        is_split_into_words=True,\n",
    "                        max_length=max_length)\n",
    "\n",
    "#train_labels = preprocess_labels(train_inputs, train_labels)\n",
    "#validation_labels = preprocess_labels(validation_inputs, val_labels)\n",
    "#test_labels = preprocess_labels(test_inputs, test_labels)\n",
    "\n",
    "train_labels = tr_tsBERT_labels\n",
    "#validation_labels = val_tsBERT_labels\n",
    "validation_labels = te_tsBERT_labels\n",
    "test_labels = te_tsBERT_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64582dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the tokenized input and label sets into a format suitable for training (datasets)\n",
    "train_dataset = STTDataset(train_inputs, train_labels)\n",
    "validation_dataset = STTDataset(validation_inputs, validation_labels)\n",
    "test_dataset = STTDataset(test_inputs, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8faa1f",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43f914f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameter grid\n",
    "recommended_learning_rates = [1e-4, 5e-5, 3e-5] # BERT authors recommend 3e-4, 1e-4, 5e-5, 3e-5\n",
    "recommended_batch_sizes = [8, 16, 32] # BERT authors recommend 8, 16, 32, 64, 128\n",
    "recommended_num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72869a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, validation_loader, num_epochs, optimizer, loss_function, device):\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    print(\"Training\")\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # train/fine-tune the model\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Training loop\n",
    "        for batch in tqdm(train_loader):\n",
    "            \n",
    "            # forward pass\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # clear previous gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "            # compute loss and backpropagate\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        print(\"Evaluating on validation set\")\n",
    "        \n",
    "        # Validation loop\n",
    "        validation_results = evaluate(model, validation_loader)\n",
    "        validation_results['epoch'] = epoch\n",
    "        validation_results['avg_train_loss'] = avg_train_loss\n",
    "\n",
    "        print(f\"Train Loss: {validation_results['avg_train_loss']}, Val Loss: {validation_results['avg_test_loss']}\")\n",
    "        \n",
    "        results.append(validation_results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f98e168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "\n",
    "    # evaluate the model\n",
    "    model.eval()\n",
    "\n",
    "    # initialize lists to store predictions and true labels\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    total_loss = 0\n",
    "\n",
    "    # disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "\n",
    "            # forward pass, get predictions\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # apply softmax to convert logits to probabilities\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # iterate over each record in the batch\n",
    "            for i in range(logits.shape[0]): # logits.shape[0] is the batch size\n",
    "                # retrieve the probas\n",
    "                record_probas = probabilities[i]\n",
    "                record_predictions = []\n",
    "\n",
    "                # keep track of the true labels corresponding to the predictions\n",
    "                record_true_labels = labels[i][labels[i] != -100]  # exclude special tokens\n",
    "                true_labels.append(record_true_labels.cpu().numpy())\n",
    "\n",
    "                # iterate over each token\n",
    "                for token_idx, label in enumerate(labels[i]):\n",
    "                    if (label == -100):\n",
    "                        continue\n",
    "                    # get the most probable class and its score for this token\n",
    "                    token_probas = record_probas[token_idx]\n",
    "                    max_idx = torch.argmax(token_probas).item()\n",
    "                    max_proba = token_probas[max_idx].item()\n",
    "\n",
    "                    record_predictions.append([model.config.id2label[max_idx], max_proba])\n",
    "\n",
    "                predictions.append(record_predictions)\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    \n",
    "    # flatten the predictions and convert them to binary format\n",
    "    predicted_labels = [0 if pred[0] == 'E' else 1 for record_predictions in predictions for pred in record_predictions]\n",
    "    flattened_true_labels = [label for record_labels in true_labels for label in record_labels]\n",
    "\n",
    "    # ensure that true and predicted labels are correctly aligned\n",
    "    if len(predicted_labels) != len(flattened_true_labels):\n",
    "        raise ValueError(\"The length of predicted labels and true labels must be the same.\")\n",
    "        \n",
    "    # compute evaluation metrics\n",
    "    accuracy = accuracy_score(flattened_true_labels, predicted_labels)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(flattened_true_labels, predicted_labels, average='binary')\n",
    "\n",
    "\n",
    "    return {'avg_test_loss': avg_loss, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c396b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 1/4\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                          | 1/340 [00:05<33:13,  5.88s/it]"
     ]
    }
   ],
   "source": [
    "# storage for all model results\n",
    "all_results = []\n",
    "\n",
    "# hyperparameter Tuning Loop\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        # initialize model, optimizer, and loss function for each combination\n",
    "        model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "        optimizer = AdamW(model.parameters(), lr=lr)\n",
    "        loss_function = CrossEntropyLoss(ignore_index=-100)\n",
    "        \n",
    "        # GPU Acceleration\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = model.to(device)\n",
    "\n",
    "        # prepare the DataLoaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        validation_loader = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "        # train and validate the model\n",
    "        results = train_and_validate(model, train_loader, validation_loader, num_epochs, optimizer, loss_function, device)\n",
    "        \n",
    "        # store results\n",
    "        all_results.append([lr, batch_size, results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the list of results to a pandas DataFrame\n",
    "all_results_df = pd.DataFrame.from_records([{\n",
    "    'learning_rate': lr,\n",
    "    'batch_size': bs,\n",
    "    'epoch': r['epoch'],\n",
    "    'train_loss': r['avg_train_loss'],\n",
    "    'val_loss': r['avg_test_loss'],\n",
    "    'val_accuracy': r['accuracy'],\n",
    "    'val_precision': r['precision'],\n",
    "    'val_recall': r['recall'],\n",
    "    'val_f1': r['f1']\n",
    "} for lr, bs, result_list in all_results for r in result_list])\n",
    "\n",
    "# save to CSV\n",
    "all_results_df.to_csv('../data/' + output_filename + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aebb72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b33cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_pretrained(\"./finetuned_tsbert_lr_\" + str(lr) + \"batch_size_\" + str(batch_size))\n",
    "#tokenizer.save_pretrained(\"./finetuned_tsbert_lr_\" + str(lr) + \"batch_size_\" + str(batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
