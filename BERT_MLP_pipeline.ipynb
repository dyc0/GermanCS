{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-MLP MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a sample pipeline that shows the BERT-MLP model used in the project. The model given here deviates from the baseline as it has weights. As multiple parameter configurations were tested in the paper, the places in which code should be changed to implement another variant of this model are denoted and instructions are given on how to obtain all of the variations that were tested. This configuration was chosen as it was most performant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global modules import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random as rnd\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local modules import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loading import create_word_lists, tidy_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/corpus_data.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "data = data[\"records\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract whole trasncripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_transcripts = [entry[\"human_transcript\"] for entry in data]\n",
    "stt_transcripts = [entry[\"stt_transcript\"] for entry in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract individual words and their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_words, stt_words, word_labels, word_grams, word_sems = create_word_lists(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the sentences are too long, so we need to shorten them. The sentences are basically concatenations of individual words with spaces in between, without any interpuction, so they are reconstructed from word lists when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stt_transcripts, stt_words, word_labels, word_grams, word_sems = tidy_sentence_length(\n",
    "    stt_transcripts, stt_words, word_labels, word_grams, word_sems\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, instead of loading the original corpus, augmented data can also be used. File `translation.py` contains a function that can be used to produce more German words in the dataset by usage of Google Translate API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE START\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to extract which sentences contain German words in order to stratify the data split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(map(len, word_labels))\n",
    "padded_labels = [row + [False] * (max_length - len(row)) for row in word_labels]\n",
    "padded_labels = np.array(padded_labels)\n",
    "stat_labels = np.any(padded_labels, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we split only indices and not data itself, because the data contains arrays of variable length, which does not work with `train_test_split`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(stt_transcripts)))\n",
    "tr_indices, te_indices = train_test_split(\n",
    "    indices, test_size=0.2, random_state=0, shuffle=True, stratify=stat_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are hepler functions that will extract data selected by indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_train = itemgetter(*tr_indices)\n",
    "extract_test = itemgetter(*te_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, do data splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_stt_transcripts = extract_train(stt_transcripts)\n",
    "tr_stt_words = extract_train(stt_words)\n",
    "\n",
    "tr_word_labels = extract_train(word_labels)\n",
    "tr_word_grams = extract_train(word_grams)\n",
    "tr_word_sems = extract_train(word_sems)\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "te_stt_transcripts = extract_test(stt_transcripts)\n",
    "te_stt_words = extract_test(stt_words)\n",
    "\n",
    "te_word_labels = extract_test(word_labels)\n",
    "te_word_grams = extract_test(word_grams)\n",
    "te_word_sems = extract_test(word_sems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dyco/EPFL/SEMESTAR_1/ML/ml-project-2-machinesoflearning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_encoder import encode_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are here using pretrained base models, but other ones can be chosen, too. Using the multilingual model did not yield any benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_bert = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
    "model_bert.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_stt_vectors = []\n",
    "te_stt_vectors = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence, words in zip(tr_stt_transcripts, tr_stt_words):\n",
    "    tr_stt_vectors.append(encode_sentence(sentence, words, model_bert, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence, words in zip(te_stt_transcripts, te_stt_words):\n",
    "    te_stt_vectors.append(encode_sentence(sentence, words, model_bert, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encodnigs in the previous step can be changed by passing `vectorization` parameter to the `encode_sentence` function. By default, encoding is done by summing the last four hidden layers of the BERT model. If `vectorization` is set to `stl`, second-to-last layer is used as representation, and if `concat`, the last four layers are concatenated to obtain the final representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the corpus to tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_tensor = torch.vstack(tr_stt_vectors)\n",
    "tr_label_tensor = torch.tensor(\n",
    "    [int(element) for sublist in tr_word_labels for element in sublist]\n",
    ")\n",
    "tr_grams_tensor = torch.tensor(\n",
    "    [int(element) for sublist in tr_word_grams for element in sublist]\n",
    ")\n",
    "tr_sems_tensor = torch.tensor(\n",
    "    [int(element) for sublist in tr_word_sems for element in sublist]\n",
    ")\n",
    "\n",
    "\n",
    "te_tensor = torch.vstack(te_stt_vectors)\n",
    "te_label_tensor = torch.tensor(\n",
    "    [int(element) for sublist in te_word_labels for element in sublist]\n",
    ")\n",
    "te_grams_tensor = torch.tensor(\n",
    "    [int(element) for sublist in te_word_grams for element in sublist]\n",
    ")\n",
    "te_sems_tensor = torch.tensor(\n",
    "    [int(element) for sublist in te_word_sems for element in sublist]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For quicker experimenting, load saved data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp import MLP, cross_validate_model, train_model, calc_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use CUDA accelleration if possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we explore the inpact of number of neurons per layer on our result. The rest of the parameters were chosen based on previous grid searched. That can be changed to explore the full hyperparameter space if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "hidden_layers = 1\n",
    "neurons_per_layer_options = [32, 64, 128, 256, 512, 700, 1024, 2048]\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float(\"inf\")\n",
    "best_param = None\n",
    "\n",
    "features = tr_tensor\n",
    "labels = tr_label_tensor\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "german_proportion = tr_label_tensor.to(torch.float).mean()\n",
    "weights = torch.tensor([1 / (1 - german_proportion), 1 / german_proportion])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class weights in the previous cell were calculated so that a less prominent class has more weight on the result of loss calculation.\n",
    "\n",
    "Features can be augmented to add a column representing grammar or semantic errors, or model perplexity score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a temporary array to store intermediate data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a grid search for best number of neurons per layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [06:23<00:00, 47.93s/it]\n"
     ]
    }
   ],
   "source": [
    "for neurons_per_layer in tqdm(neurons_per_layer_options):\n",
    "    model = MLP(features.shape[1], hidden_layers, neurons_per_layer).to(torch_device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    tr_loss, tr_loss_std, te_loss, te_loss_std = cross_validate_model(\n",
    "        model,\n",
    "        features,\n",
    "        labels,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        splitter,\n",
    "        n_epochs=epochs,\n",
    "        num_workers=0,\n",
    "        device=torch_device,\n",
    "        class_weights=weights,\n",
    "    )\n",
    "\n",
    "    values_to_add = [neurons_per_layer, tr_loss, tr_loss_std, te_loss, te_loss_std]\n",
    "\n",
    "    # Add preliminary data to dataframe\n",
    "    grid_search_data.append(values_to_add)\n",
    "\n",
    "    if te_loss < best_loss:\n",
    "        best_loss = te_loss\n",
    "        best_param = neurons_per_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above loop can be redesigned to perform bias-variance tradeoff testing, by splitting the dataset into multiple subsets and using their growing unions as data for cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe to store hyperparameter data and save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \".\"\n",
    "columns = [\"neurons_per_layer\", \"tr_loss\", \"tr_loss_std\", \"te_loss\", \"te_loss_std\"]\n",
    "gs_frame = pd.DataFrame(grid_search_data, columns=columns)\n",
    "gs_frame.to_csv(os.path.join(out_path, \"gs_data_test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp import STTDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model on the whole dataset with the best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = STTDataset(tr_tensor, tr_label_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "german_proportion = tr_label_tensor.to(torch.float).mean()\n",
    "weights = torch.tensor([1 / (1 - german_proportion), 1 / german_proportion])\n",
    "neurons_per_layer = best_param\n",
    "\n",
    "criterion = nn.BCELoss(reduction=\"none\")\n",
    "model = MLP(train_data.embeddings.shape[1], hidden_layers, neurons_per_layer).to(\n",
    "    torch_device\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10087998536274866"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    n_epochs=epochs,\n",
    "    device=torch_device,\n",
    "    class_weights=weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = STTDataset(te_tensor, te_label_tensor)\n",
    "test_loader = DataLoader(\n",
    "    test_data, batch_size=len(test_data), shuffle=True, num_workers=0\n",
    ")\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(torch_device), labels.to(torch_device)\n",
    "        pred = model(inputs)\n",
    "        pred = torch.squeeze(pred, dim=1)\n",
    "        loss = criterion(pred, labels.to(torch.float)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1 = calc_stats(pred, te_label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03375527426160338"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    [[loss, accuracy, precision, recall, f1]],\n",
    "    columns=[\"loss\", \"accuracy\", \"precision\", \"recall\", \"f1\"],\n",
    ")\n",
    "results.to_csv(os.path.join(out_path, \"results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how well the model performs on german words by extracting them and their assigned labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_te_words = [element for sublist in te_stt_words for element in sublist]\n",
    "all_te_labels = [element for sublist in te_word_labels for element in sublist]\n",
    "all_te_predictions = (pred.to(\"cpu\").numpy().flatten() > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_words = []\n",
    "german_predictions = []\n",
    "for i in range(len(all_te_words)):\n",
    "    if all_te_labels[i]:\n",
    "        german_words.append(all_te_words[i])\n",
    "        german_predictions.append(all_te_predictions[i])\n",
    "\n",
    "predicted_labels = pd.DataFrame(\n",
    "    {\"word\": german_words, \"prediction\": german_predictions}\n",
    ")\n",
    "predicted_labels.to_csv(os.path.join(out_path, \"word_labels.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>ma</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>i</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>on</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>to</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>spear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>salton</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>site</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>as</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>s</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>he</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>me</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>freshen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>in</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>do</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>staaken</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>wot</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>the</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>we</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>stagei</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  prediction\n",
       "156       ma           1\n",
       "66         i           1\n",
       "331       on           1\n",
       "257       to           1\n",
       "35     spear           1\n",
       "128   salton           1\n",
       "320     site           1\n",
       "41        as           1\n",
       "386        s           1\n",
       "122       he           1\n",
       "360       me           1\n",
       "330  freshen           1\n",
       "146       in           1\n",
       "214       do           1\n",
       "61        10           1\n",
       "262  staaken           1\n",
       "264      wot           0\n",
       "271      the           0\n",
       "259       we           0\n",
       "260   stagei           0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels.sort_values(by=\"prediction\", ascending=False).head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
